{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extending torch.autograd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLOFzXKtFo3F2YvzqOUghL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristinaMarsh/Learning_/blob/main/Pytorch/Extending_torch_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\n",
        "\n",
        "\n",
        "# How to use\n",
        "Take the following steps: \n",
        "\n",
        "1. Subclass Function and implement the `forward()` and `backward()` methods. \n",
        "2. Call the proper methods on the **ctx** argument. \n",
        "3. Declare whether your function supports double backward. \n",
        "4. Validate whether your gradients are correct using `gradcheck`\n",
        "\n",
        "\n",
        "Step 1: After subclassing Function, you'll need to define 2 methods:\n",
        "- forward () is the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here. Tensor arguments that track history (i.e., with requires_grad=True ) will be converted to ones that don't track history before the call, and their use will be registered in the graph. Note that this logic won't traverse lists/dicts/any other data structures and will only consider tensors that are direct arguments to the call. You can return either a single Tensor output, or a tuple of tensors if there are multiple outputs. Also, please refer to the docs of Function to find descriptions of useful methods that can be called only from forward().\n",
        "- backward () (or $\\operatorname{vjp}())$ defines the gradient formula. It will be given as many Tensor arguments as there were outputs, with each of them representing gradient w.r.t. that output. It is important NEVER to modify these in-place. It should return as many tensors as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn't require gradient (needs_input_grad is a tuple of booleans indicating whether each input needs gradient computation), or were non- Tensor objects, you can return None. Also, if you have optional arguments to forward () you can return more gradients than there were inputs, as long as they're all None.\n",
        "\n",
        "Step 2: It is your responsibility to use the functions in the forward's ctx properly in order to ensure that the new Function works properly with the autograd engine.\n",
        "- save_for_backward() must be used when saving input or output tensors of the forward to be used later in the backward. Anything else, i.e., non-tensors and tensors that are neither input nor output should be stored directly on ctx.\n",
        "- mark_dirty() must be used to mark any input that is modified inplace by the forward function.\n",
        "- mark_non_differentiable() must be used to tell the engine if an output is not differentiable. By default all output tensors that are of differentiable type will be set to require gradient. Tensors of non-differentiable type (i.e., integral types) are never marked as requiring gradients.\n",
        "- set_materialize_grads() can be used to tell the autograd engine to optimize gradient computations in the cases where the output does not depend on the input by not materializing grad tensors given to backward function. That is, if set to False, None object in python or \"undefined tensor\" (tensor x for which x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior to calling backward, and so your code will need to handle such objects as if they were tensors filled with zeros. The default value of this setting is True.\n",
        "\n",
        "Step 3: If your Function does not support double backward you should explicitly declare this by decorating backward with the once_differentiable(). With this decorator, attempts to perform double backward through your function will produce an error. See our double backward tutorial for more information on double backward.\n",
        "\n",
        "Step 4: It is recommended that you use torch. autograd. gradcheck() to check whether your backward function correctly computes gradients of the forward by computing the Jacobian matrix using your backward function and comparing the value element-wise with the Jacobian computed numerically using finite-differencing.\n"
      ],
      "metadata": {
        "id": "Q7AXQwiZgR8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "Below you can find code for a Linear function from `torch.nn`, with additional comments:"
      ],
      "metadata": {
        "id": "ytZ-b8lohYbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XJ5WF_AngOuk"
      },
      "outputs": [],
      "source": [
        "# Inherit from Function\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "class LinearFunction(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    # bias is an optional argument\n",
        "    def forward(ctx, input, weight, bias=None):\n",
        "        ctx.save_for_backward(input, weight, bias)\n",
        "        output = input.mm(weight.t())\n",
        "        if bias is not None:\n",
        "            output += bias.unsqueeze(0).expand_as(output)\n",
        "        return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # This is a pattern that is very convenient - at the top of backward\n",
        "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
        "        # None. Thanks to the fact that additional trailing Nones are\n",
        "        # ignored, the return statement is simple even when the function has\n",
        "        # optional inputs.\n",
        "        input, weight, bias = ctx.saved_tensors\n",
        "        grad_input = grad_weight = grad_bias = None\n",
        "\n",
        "        # These needs_input_grad checks are optional and there only to\n",
        "        # improve efficiency. If you want to make your code simpler, you can\n",
        "        # skip them. Returning gradients for inputs that don't require it is\n",
        "        # not an error.\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input)\n",
        "        if bias is not None and ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0)\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear = LinearFunction.apply"
      ],
      "metadata": {
        "id": "CsrslOiZjXZ6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MulConstant(Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, tensor, constant):\n",
        "    ctx.constant = constant\n",
        "    return tensor * constant\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    return grad_output * ctx.constant, None"
      ],
      "metadata": {
        "id": "dd1k3IaWjjwy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we give an additional example of a function that is parametrized by non-Tensor arguments:\n",
        "\n",
        "class MulConstant(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, tensor, constant):\n",
        "        # ctx is a context object that can be used to stash information\n",
        "        # for backward computation\n",
        "        ctx.constant = constant\n",
        "        return tensor * constant\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # We return as many input gradients as there were arguments.\n",
        "        # Gradients of non-Tensor arguments to forward must be None.\n",
        "        return grad_output * ctx.constant, None"
      ],
      "metadata": {
        "id": "bB15CKC2jdh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And here, we optimize the above example by calling set_materialize_grads(False):\n",
        "class MulConstant(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, tensor, constant):\n",
        "        ctx.set_materialize_grads(False)\n",
        "        ctx.constant = constant\n",
        "        return tensor * constant\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Here we must handle None grad_output tensor. In this case we\n",
        "        # can skip unnecessary computations and just return None.\n",
        "        if grad_output is None:\n",
        "            return None, None\n",
        "\n",
        "        # We return as many input gradients as there were arguments.\n",
        "        # Gradients of non-Tensor arguments to forward must be None.\n",
        "        return grad_output * ctx.constant, None"
      ],
      "metadata": {
        "id": "nM2V949RkDWV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\n",
        "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxALaIs_knHM",
        "outputId": "dae4aca1-492c-46a1-e7fb-df712f7fb9d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You probably want to check if the backward method you implemented actually computes the derivatives of your function. It is possible by comparing with numerical approximations using small finite differences:"
      ],
      "metadata": {
        "id": "L3S1JTQpkfO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
        "# evaluated with these tensors are close enough to numerical\n",
        "# approximations and returns True if they all verify this condition.\n",
        "input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\n",
        "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKd5bvbpkGgq",
        "outputId": "260d5bfe-0740-424b-b625-2d4ef19bbbff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}